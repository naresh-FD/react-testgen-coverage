{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naresh-FD/react-testgen-coverage/blob/main/react_testgen_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J5pMuUDEerAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6027499f-6439-4e6d-a9dc-abe83a27007e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m530.9/530.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"torch>=2.1\" transformers datasets peft accelerate bitsandbytes trl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsrT0KmX-npa",
        "outputId": "aa38db5f-06fc-4686-c5ac-5f7395effaff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nBNXsTF_NXb",
        "outputId": "c50aedba-a69c-4928-db1a-a7e34ec08e15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 406K\n",
            "drwx------ 2 root root 4.0K Feb 10 05:49 'Colab Notebooks'\n",
            "drwx------ 2 root root 4.0K Jan 31 06:16  Documents\n",
            "drwx------ 2 root root 4.0K Feb  1 05:36 'Exchange events attachments'\n",
            "-rw------- 1 root root 390K Feb 10 05:46  react_testgen_train.ipynb\n",
            "drwx------ 2 root root 4.0K Jan 31 06:16  Social\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/training.jsonl /content/training.jsonl"
      ],
      "metadata": {
        "id": "GdrdTK-8_rCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0824ac18-594c-410d-ac58-e1dff20b6552"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/training.jsonl': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content\n",
        "!head -n 2 /content/training.jsonl\n"
      ],
      "metadata": {
        "id": "FynEhxvX_4_U",
        "outputId": "4a947243-4c6c-4653-a8a1-e1aacc6e85b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8.0K\n",
            "drwx------ 5 root root 4.0K Feb 10 05:51 drive\n",
            "drwxr-xr-x 1 root root 4.0K Dec  9 14:42 sample_data\n",
            "head: cannot open '/content/training.jsonl' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/training.jsonl\"\n",
        ")[\"train\"]\n",
        "\n",
        "print(\"Total examples:\", len(dataset))\n",
        "print(dataset[0][\"messages\"][1][\"content\"][:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8IVPc5K8ANF7",
        "outputId": "f2abfd87-ae54-476a-d2db-5523eca0da54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Unable to find '/content/training.jsonl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2457591732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dataset = load_dataset(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/training.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         )\n\u001b[0;32m--> 526\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 582\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    583\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/content/training.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.column_names)\n",
        "print(dataset[0][\"messages\"][0])\n",
        "print(dataset[0][\"messages\"][1])\n"
      ],
      "metadata": {
        "id": "vYP5cqCdBFGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_text(ex):\n",
        "    # assumes messages = [{role, content}, {role, content}, ...]\n",
        "    msgs = ex[\"messages\"]\n",
        "    # minimal format: user prompt -> assistant answer\n",
        "    # if you have system+user+assistant, this still works well\n",
        "    user_parts = [m[\"content\"] for m in msgs if m[\"role\"] in [\"user\", \"system\"]]\n",
        "    assistant_parts = [m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"]\n",
        "\n",
        "    prompt = \"\\n\\n\".join(user_parts).strip()\n",
        "    completion = \"\\n\\n\".join(assistant_parts).strip()\n",
        "\n",
        "    return {\"text\": f\"{prompt}\\n\\n### RESPONSE:\\n{completion}\"}\n",
        "\n",
        "dataset_text = dataset.map(to_text)\n",
        "print(dataset_text[0][\"text\"][:600])\n"
      ],
      "metadata": {
        "id": "9UcrP0w5BMdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"torch>=2.1\" transformers datasets peft accelerate bitsandbytes trl\n"
      ],
      "metadata": {
        "id": "XTQib_ncBTX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n"
      ],
      "metadata": {
        "id": "iSK2zMtfBWLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "4ZHb4b3KBZjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(ex):\n",
        "    out = tok(\n",
        "        ex[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        padding=False,\n",
        "    )\n",
        "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
        "    return out\n",
        "\n",
        "train_ds = dataset_text.map(tokenize, remove_columns=dataset_text.column_names)\n",
        "print(train_ds[0].keys())\n"
      ],
      "metadata": {
        "id": "iUj78f0BBcoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=5,          # small dataset => more epochs ok\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    bf16=True, # Changed from fp16=True\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "R3gU_vvzCGUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/react-testgen-lora\")\n",
        "tok.save_pretrained(\"/content/react-testgen-lora\")\n",
        "!ls -lh /content/react-testgen-lora\n"
      ],
      "metadata": {
        "id": "od-K9HwACTPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate(prompt: str):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=600,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompt = dataset_text[0][\"text\"].split(\"### RESPONSE:\")[0].strip() + \"\\n\\n### RESPONSE:\\n\"\n",
        "print(generate(test_prompt))\n"
      ],
      "metadata": {
        "id": "wux4ieEyCuGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_text(ex):\n",
        "    msgs = ex[\"messages\"]\n",
        "    # Combine system+user into prompt, assistant into completion\n",
        "    prompt = \"\\n\\n\".join([m[\"content\"] for m in msgs if m[\"role\"] in [\"system\",\"user\"]]).strip()\n",
        "    completion = \"\\n\\n\".join([m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"]).strip()\n",
        "    return {\"text\": f\"{prompt}\\n\\n### RESPONSE:\\n{completion}\"}\n",
        "\n",
        "dataset_text = dataset.map(to_text)\n",
        "print(dataset_text[0][\"text\"][:600])\n"
      ],
      "metadata": {
        "id": "xAeNG7BzLEhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66cc10c6"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a sample dataset (e.g., a small subset of alpaca)\n",
        "# Alpaca dataset structure: 'instruction', 'input', 'output'\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:50]\") # Load first 50 examples\n",
        "\n",
        "print(\"Total examples:\", len(dataset))\n",
        "# Print an example of the new dataset's content\n",
        "print(dataset[0][\"instruction\"][:300])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "869f3d51"
      },
      "source": [
        "def to_text(ex):\n",
        "    prompt_parts = []\n",
        "    if ex[\"instruction\"]:\n",
        "        prompt_parts.append(ex[\"instruction\"])\n",
        "    if ex[\"input\"]:\n",
        "        prompt_parts.append(f\"Input: {ex['input']}\")\n",
        "\n",
        "    prompt = \"\\n\\n\".join(prompt_parts).strip()\n",
        "    completion = ex[\"output\"].strip()\n",
        "\n",
        "    return {\"text\": f\"{prompt}\\n\\n### RESPONSE:\\n{completion}\"}\n",
        "\n",
        "dataset_text = dataset.map(to_text)\n",
        "print(dataset_text[0][\"text\"][:600])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"torch>=2.1\" transformers datasets peft accelerate bitsandbytes trl\n"
      ],
      "metadata": {
        "id": "ijdrQWFLMuIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ad3dcc"
      },
      "source": [
        "def tokenize(ex):\n",
        "    out = tok(\n",
        "        ex[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        padding=False,\n",
        "    )\n",
        "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
        "    return out\n",
        "\n",
        "train_ds = dataset_text.map(tokenize, remove_columns=dataset_text.column_names)\n",
        "print(train_ds[0].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ebf0a7"
      },
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=5,          # small dataset => more epochs ok\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    bf16=True, # Changed from fp16=True\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "080a1dd8"
      },
      "source": [
        "model.save_pretrained(\"/content/react-testgen-lora\")\n",
        "tok.save_pretrained(\"/content/react-testgen-lora\")\n",
        "!ls -lh /content/react-testgen-lora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb79f2fe"
      },
      "source": [
        "import torch\n",
        "\n",
        "def generate(prompt: str):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=600,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompt = dataset_text[0][\"text\"].split(\"### RESPONSE:\")[0].strip() + \"\\n\\n### RESPONSE:\\n\"\n",
        "print(generate(test_prompt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n"
      ],
      "metadata": {
        "id": "NMCO3hMbMuDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "bnb = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "3SqxYLWbM4qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ZUi8fX0GNB3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Make sure you've uploaded training.jsonl to Colab first\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/training.jsonl\", split=\"train\")\n",
        "\n",
        "print(f\"âœ… Loaded {len(dataset)} training examples\")\n",
        "print(f\"Sample keys: {dataset[0].keys()}\")\n",
        "print(f\"Messages in first example: {len(dataset[0]['messages'])}\")\n"
      ],
      "metadata": {
        "id": "zzKz1N57qbUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat(example):\n",
        "    \"\"\"Convert messages array to ChatML format string\"\"\"\n",
        "    text = \"\"\n",
        "    for msg in example[\"messages\"]:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            text += f\"<|im_start|>system\\n{msg['content']}<|im_end|>\\n\"\n",
        "        elif msg[\"role\"] == \"user\":\n",
        "            text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "formatted_dataset = dataset.map(format_chat)\n",
        "print(f\"âœ… Formatted {len(formatted_dataset)} examples\")\n",
        "print(f\"Sample text length: {len(formatted_dataset[0]['text'])} chars\")"
      ],
      "metadata": {
        "id": "1Z9h_n6Trr5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/output\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    max_grad_norm=0.3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "print(\"âœ… Training args configured\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")"
      ],
      "metadata": {
        "id": "_PDrn6t5ru3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Move all args into SFTConfig (replaces both TrainingArguments and SFTTrainer params)\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"/content/output\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    bf16=True, # Changed from fp16=True\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    max_grad_norm=0.3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_dataset,\n",
        "    args=sft_config\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")"
      ],
      "metadata": {
        "id": "eHRMnq6Mrunh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Save LoRA adapter\n",
        "trainer.save_model(\"/content/output/final\")\n",
        "tok.save_pretrained(\"/content/output/final\")\n",
        "\n",
        "# Merge LoRA weights back into base model\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"/content/output/final\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"/content/output/merged\", safe_serialization=True)\n",
        "tok.save_pretrained(\"/content/output/merged\")\n",
        "\n",
        "print(\"âœ… Model merged and saved!\")"
      ],
      "metadata": {
        "id": "5nuTiGdgswI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install llama.cpp converter\n",
        "!pip install -q llama-cpp-python\n",
        "\n",
        "# Clone llama.cpp for conversion script\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp /content/llama_cpp\n",
        "\n",
        "# Install requirements\n",
        "!pip install -q -r /content/llama_cpp/requirements.txt\n",
        "\n",
        "# Convert to GGUF\n",
        "!python /content/llama_cpp/convert_hf_to_gguf.py \\\n",
        "    /content/output/merged \\\n",
        "    --outfile /content/react-testgen.gguf \\\n",
        "    --outtype q4_K_M\n",
        "\n",
        "import os\n",
        "size_mb = os.path.getsize(\"/content/react-testgen.gguf\") / (1024*1024)\n",
        "print(f\"âœ… GGUF file created: {size_mb:.0f} MB\")\n",
        "print(\"ðŸ“¥ Download: Click Files panel â†’ right-click react-testgen.gguf â†’ Download\")\n"
      ],
      "metadata": {
        "id": "dW2RM3vMt6-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}